---
chapter: 5
knit: "bookdown::render_book"
---

# Classification of ignition causes {#ch:classifier}

## Model description

We use a random forest model to classify historical bushfire ignitions. A random forest [@breiman2001random] is an ensemble learning method for building tree-based prediction models. It is perhaps one of the most regularly used machine learning models in various fields [@boulesteix2012overview; @heung2016overview]. It combines the predictions of many decision trees constructed using the bootstrapping the sample and randomly selecting variables/features at each tree node, takes the majority vote as the forest prediction. A random forest model guards against overfitting of the training set by only using out-of-bag predictions from each tree.

Other candidate models we examine in this research are the multinomial logit model [@berkson1944application], the generalised additive model (GAM) multinomial logistic regression [@yee1996vector] and the XGBoost [@chen2016xgboost]. 

A multinomial logit model [@berkson1944application] is a generalization of the logistic regression to multi-class problems, which is commonly used as the baseline model in predictive modelling. 

A generalised additive model [@hastie1990generalized] is a generalised linear model with additive smooths terms in the link function and GAM multinomial logistic regression [@yee1996vector] is its extension to multi-class problems. The generalised additive model is relatively popular in the field of bushfire ignition analysis. Some examples are @bates2018exploratory, @read2018lightning and @zhang2017wildfire. 

An XGBoost [@chen2016xgboost] is an open source distributed gradient boosting library. It provides a parallel tree boosting to solve complex regression and classification problems efficiently. Gradient boosting [@gboost] is an important technique in machine learning, which belongs to the class of boosting algorithms. It is a method to build a strong learner, which often referred to as an ensemble model, by aggregating a set of week learners iteratively. Numerous competitions, for example, the Higgs boson machine learning challenge [@adam2015higgs] and the Global energy forecasting competition 2012 [@hong2014global], have shown that XGBoost is one of the dominant methods in building prediction models on structured data.


In this research, the model building process includes feature selection, hyperparameter tuning and candidate model selection. These are described in details in the following sections. 

The multinomial logit model, generalised additive model, random forest and XGboost are available in package `nnet` [@R-nnet], `mgcv` [@R-mgcv], `randomForest` [@R-rf] and `xgboost` [@R-xgboost] respectively. Besides, package `lime` [@R-lime] is used to perform feature selection and package `caret` [@R-caret] is used to control the training, hyperparameter tuning and candidate model selection. 

In terms of the train-test split, we randomly select 80% of total data as the training set, and the rest 20% data is test set. The total number of training samples is 7497 and the total number of test samples is 1872. 


## Feature selection


In feature selection, a reasonable principle is to select the most important features. Concerning the variable importance, @strobl2007bias in their research has shown that the global variable importance, particularly random forest variable importance, can be bias and misleading. Alternatively, unlike the global variable importance provided by many other packages, the package `lime` [@R-lime] provides the local variable importance under the assumption that machine learning models are linear at the local scope. 

Given an observation, `lime` randomly samples data points around the predictors, and obtains their predictions by passing them into the machine learning model. It then fits a simple model, typically a ridge regression [@hoerl1970ridge] or a lasso regression [@tibshirani1996regression], on these data points. Due to the characteristic of the lasso regression, it is possible to select the most important variables based on the regularization path. By repeating this process for large enough observations, variables being frequently selected are the most important variables. Figure \ref{fig:limeex} is an example of the result produced by `lime`.

The strategic used in this research to perform feature selection is first fitting and tuning a full model with all covariates using 3-fold cross-validation grid searching controlled by `caret` [@R-caret], then passing in 100 observations for `lime` to find the top 10 most important variables. The final set of variables selected for each candidate model is given in Table \ref{tab:limemost}. Table \ref{tab:covshort} gives detail information about each variable. From the result, we find the characteristics of different candidate models in ignition method classification. Simpler models, such as the multinomial logistic regression model and the GAM multinomial logistic regression model are preferred to use climate covariates, while the random forest model and the XGBoost model rely on ignition location and anthropogenic covariates.

```{r}
datasets_info = data.frame(index = c("lon",
                                     "lat",
                                     "FOR_TYPE",
                                     "COVER",
                                     "HEIGHT",
                                     "arf360",
                                     "ase90",
                                     "ase180",
                                     "ase720",
                                     "amaxt90",
                                     "amaxt180",
                                     "amaxt720",
                                     "amint180",
                                     "ws",
                                     "aws_m12",
                                     "aws_m24",
                                     "log_dist_cfa",
                                     "log_dist_camp",
                                     "log_dist_road"),
                           name = c("Longitude",
                                    "Latitude",
                                    "Forest type. Eg. Acacia, Callitris, Casuarina, etc.",
                                    "Forest crown cover",
                                    "Forest height class",
                                    "Average rainfall in the past 360 days",
                                    "Average global solar exposure in the past 90 days",
                                    "Average global solar exposure in the past 180 days",
                                    "Average global solar exposure in the past 720 days",
                                    "Average maximum temperature in the past 90 days",
                                    "Average maximum temperature in the past 180 days",
                                    "Average maximum temperature in the past 720 days",
                                    "Average minimum temperature in the past 180 days",
                                    "Average wind speed on that day",
                                    "Average wind speed in last 12 months",
                                    "Average wind speed in last 24 months",
                                    "Natural logarithm of the distance to the nearest CFA station",
                                    "Natural logarithm of the distance to the nearest recreation site",
                                    "Natural logarithm of the distance to the nearest road"), 
                           type = c("degrees",
                                     "degrees",
                                     "",
                                     "",
                                     "",
                                     "mm",
                                     "MJ/m^2",
                                     "MJ/m^2",
                                     "MJ/m^2",
                                     "Celsius degree",
                                     "Celsius degree",
                                     "Celsius degree",
                                     "Celsius degree",
                                     "m/s",
                                     "m/s",
                                     "m/s",
                                     "m",
                                     "m",
                                     "m"))

knitr::kable(datasets_info, 'latex', 
             caption  = 'Detail information about the variables involved in the feature selection process.', 
             label = "covshort",
             booktabs = TRUE,
             col.names = c("Covariate name", "description", "Units"))  %>%
kableExtra::kable_styling(latex_options = c("scale_down"))
```



```{r limeex, fig.cap="An example of selecting the most important variables for the random forest model with respect to the predicted probability of the bushfire ignited by lightning in 4 cases using `lime`. The feature weight is the weighted importance within a case. In this example, the common feature that influences all observations is latitude (lat)."}

if (!file.exists("data/lime_ex.rds")){
  library(caret)
  set.seed(10086)
  
  # Read in training data
  training <- read_csv("data/training.csv")
  
  training <- training %>%
    filter(!CAUSE %in% c("BURNING BUILDING",
                         "WASTE DISPOSAL, INDUSTRIAL, SAWMILL, TIP",
                         "WASTE DISPOSAL, DOMESTIC",
                         "BURNING VEHICLE, MACHINE",
                         "BURNING BUILDING")) %>%
    filter(new_cause != "other") %>%
    filter(new_cause != "relight")
  
  
  training <- select(training, -c(EVENTID:FIRE_NUM), -id, -CAUSE, -FOREST, -FOR_CODE, -FOR_CAT)
  
  training <- mutate(training,
                     year = factor(year(FIRE_START)),
                     month = factor(month(FIRE_START), levels = c(10,11,12,1,2,3)),
                     day = factor(day(FIRE_START), levels = c(1:31)),
                     wod = factor(wday(FIRE_START), levels = c(1:7)))
  
  training <- filter(training, month %in% c(10,11,12,1,2,3))
  
  training <- na.omit(training)
  
  training <- mutate(training, new_cause = ifelse(new_cause == "accidental_human", "accident", new_cause)) %>%
    mutate(new_cause = ifelse(new_cause == "burning_off_human", "burning_off", new_cause)) %>%
    mutate(new_cause = factor(new_cause)) %>%
    mutate(FOR_TYPE = factor(FOR_TYPE))
  
  training <- na.omit(training)
  
  training <- mutate(training,
                     log_dist_cfa = log(dist_cfa),
                     log_dist_camp = log(dist_camp),
                     log_dist_road = log(dist_road),
                     COVER = factor(COVER),
                     HEIGHT = factor(HEIGHT))
  
  training <- rename(training, cause = new_cause)
  training <- mutate(training,
                     cause = fct_relevel(cause,
                                         "lightning",
                                         "accident",
                                         "arson",
                                         "burning_off"))
  
  training <- na.omit(training)
  
  training <- select(training, -year, -dist_road, -dist_cfa, -dist_camp, -FIRE_START)
  
  inTraining <- createDataPartition(training$cause, p = .8, list = FALSE)[,1]
  train_set <- training[inTraining,]
  test_set  <- training[-inTraining,]
  
  set.seed(123456)
  
  lime_sample <- sample(1:length(test_set$cause), 4)
  
  library(lime)
  
  rf_model <- readRDS("data/RF_model.rds")
  
  explainer <- lime(train_set, rf_model)
  
  explaination <- explain(test_set[lime_sample, ],
                          explainer,
                          n_labels = 4,
                          n_features = 5)
  
  p <- plot_explanations(filter(explaination, label == "lightning"))
  saveRDS(p, file = "data/lime_ex.rds")
  p
} else {
  readRDS("data/lime_ex.rds")
}

```

```{r}
readRDS("data/MNL_best_features.rds") %>%
  select(feature) %>%
  rename("Multinomial logistic regression" = feature) %>%
  bind_cols(readRDS("data/GAM_best_features.rds") %>%
  select(feature) %>%
  rename("GAM multinomial logistic regression" = feature)) %>%
  bind_cols(readRDS("data/RF_best_features.rds") %>%
  select(feature) %>%
  rename("Random forest" = feature)) %>%
  bind_cols(readRDS("data/XGB_best_features.rds") %>%
  select(feature) %>%
  rename("XGBoost" = feature)) %>%
  knitr::kable('latex',
               booktabs = T,
               label = 'limemost',
               caption = "The top 10 most important variables for each candidate model ranked in descending order. Variables on the top are more important. The differences in choices of variables across candidate models can be observed. Random forest and XGBoost exploit the location variables and anthropogenic variables. Vegetation factors are most influential in multinomial logistic regression. Solar exposure and wind speed are most important in GAM multinomial logistic regression. ") %>%
  kableExtra::kable_styling(latex_options = "scale_down")
  
  
  
  
  
  
```


## Hyperparameter tuning and candidate model selection

The hyperparameter tuning for each candidate model is performed by using 3-fold cross-validation grid searching controlled by package `caret` [@R-caret]. We set up a grid of potential hyperparameters and evaluate their performance cell by cell. The grid, the definition of every hyperparameter and the optimal hyperparameters is given in the Appendix.

The final step of the model building process is candidate model selection. Model performance is compared by using both overall prediction accuracy and multi-class AUC. Multi-class AUC is defined by Hand and Till [@hand2001simple] and it is available in package `pROC` [@R-pROC]. This metric generalises the commonly used AUC into multiple class classification problems by averaging pairwise comparison of classes.


## Results

After performing feature selection and parameter tuning, we find that random forest outperforms all other candidate models in both overall prediction accuracy and multi-class AUC. Thus, we choose the random forest model as our final model. Model performance is given in Table \ref{tab:accandauc}. More details about the model performance can be found in the Appendix. 

```{r}
data.frame(model = c("Multinomial logistic regression", "GAM multinomial logistic regression", "Random forest", "XGBoost"), 
           accuracy = c(0.5272, 0.6779, 0.7495, 0.7388),
           auc = c(0.7424, 0.8233, 0.8795, 0.8752)) %>%
    knitr::kable('latex',
               col.names = c("Model", "Accuracy", "Multi-class AUC"),
               label = "accandauc",
               caption = 'Performance of the candidate models. Random forest model is the best in terms of accuracy and multi-class AUC.',
               booktabs = T)
```

The overall accuracy of our model is 74.95%. The confusion matrix is shown in Table \ref{tab:conrf}. It suggests that lightning-caused and accident-caused ignitions can be easily classified from other causes. 77.9% of accident-caused and 90.5% of lightning-caused ignitions are correctly recognised by the model, which is a reliable result. Meanwhile, the model is not very confident with arson (53.8%) and burning off (23.5%). An error rate map for model diagnostic can be found in the Appendix.


```{r}
data.frame(Lightning = c(703, 51, 18, 5, 777), 
           Accident = c(77, 494, 55, 8, 634),
           Arson = c(50, 89, 175, 11, 325),
           Burning_off = c(44, 38, 22, 32, 136),
           Total = c(874, 672, 270, 56, 1872)) %>%
  mutate(Lightning = c(paste0(Lightning[1:4], " (", round(Lightning[1:4]/Lightning[5]*100, 1), "%)" ), paste0(Lightning[5]))) %>%
  mutate(Accident = c(paste0(Accident[1:4], " (", round(Accident[1:4]/Accident[5]*100, 1), "%)" ), paste0(Accident[5]))) %>%
  mutate(Arson = c(paste0(Arson[1:4], " (", round(Arson[1:4]/Arson[5]*100, 1), "%)" ), paste0(Arson[5]))) %>%
  mutate(Burning_off = c(paste0(Burning_off[1:4], " (", round(Burning_off[1:4]/Burning_off[5]*100, 1), "%)" ), paste0(Burning_off[5]))) %>%
  `row.names<-`(c("Prediction:lightning", "Prediction:accident", "Prediction:arson", "Prediction:buring_off", "Total")) %>%
  knitr::kable('latex',
               booktabs = T,
               label = "conrf",
               caption = "Confusion matrix of the random forest model. The overall accuracy is 0.7495.") 
```


<!-- High-dimensional projection produced by guided tour with LDA projection pursuit index [@cook2007interactive] and tools in package `tourr` [@R-tourr] reveals the similar findings. In Figure \ref{fig:ldalightning}, although lightning-caused ignitions can not be fully separated from other points, we can still find a reasonably good classification boundary on this projection. In contrast, it is not possible to find a clear boundary in Figure \ref{fig:ldaarson}, where almost all points mixed together. -->

<!-- ![The guided tour with LDA projection pursuit index shows that lightning-caused ignitions (green) can possibly be distinguished from other causes. \label{fig:ldalightning}](figures/lda_guide_tour_lightning.jpeg){width=275 height=275} -->

<!-- ![The guided tour with LDA projection pursuit indx shows that arson-caused ignitions (green) can not be classified from other causes. \label{fig:ldaarson}](figures/lda_guide_tour_arson.jpeg){width=275 height=275} -->




The weighted variable contribution to the probability of different causes produced by `lime` is the scaled coefficients obtained from the lasso regression at the local scope. Figure \ref{fig:limeexvar} shows proximity to the nearest CFA station and proximity to the nearest road have a high positive impact on the probability of lightning-caused bushfire, while 2-year average wind speed has a high negative impact on the probability. Patterns in arson are almost the opposite of what has been shown in lightning. Latitude and 2-year average wind speed have some positive impact on the probability of accident-caused bushfire, while average wind speed in past 12 months, proximity to the nearest road and proximity to the nearest camping site have negative to the probability. Variable contribution to the probability of planned burn is relatively small, and the proximity to the nearest CFA station and latitude contribute negatively to the probability.

For future fire investigation, if a bushfire starts at a remote area in a windless year, it is very likely to be lightning-ignited bushfire. In contrast, if the bushfire is very close to the CFA station and starts in a windy year, it is possible to be arson. Moreover, the accident-caused bushfire usually starts near the recreation site and road in a windless year after a windy year.

```{r limeexvar, fig.cap = "Variable contribution to the probability of different causes. Variable has a positive weight means it has a postive impact on the probability. The same rule applies to negative weights. The feature weight can be seen as the marginal effect at the local scope."}
library(caret)

if (!file.exists("data/lime_ex2")){
  set.seed(10086)
  
  # Read in training data
  training <- read_csv("data/training.csv")
  
  training <- training %>%
    filter(!CAUSE %in% c("BURNING BUILDING",
                         "WASTE DISPOSAL, INDUSTRIAL, SAWMILL, TIP",
                         "WASTE DISPOSAL, DOMESTIC",
                         "BURNING VEHICLE, MACHINE",
                         "BURNING BUILDING")) %>%
    filter(new_cause != "other") %>%
    filter(new_cause != "relight")
  
  
  training <- select(training, -c(EVENTID:FIRE_NUM), -id, -CAUSE, -FOREST, -FOR_CODE, -FOR_CAT)
  
  training <- mutate(training,
                     year = factor(year(FIRE_START)),
                     month = factor(month(FIRE_START), levels = c(10,11,12,1,2,3)),
                     day = factor(day(FIRE_START), levels = c(1:31)),
                     wod = factor(wday(FIRE_START), levels = c(1:7)))
  
  training <- filter(training, month %in% c(10,11,12,1,2,3))
  
  training <- na.omit(training)
  
  training <- mutate(training, new_cause = ifelse(new_cause == "accidental_human", "accident", new_cause)) %>%
    mutate(new_cause = ifelse(new_cause == "burning_off_human", "burning_off", new_cause)) %>%
    mutate(new_cause = factor(new_cause)) %>%
    mutate(FOR_TYPE = factor(FOR_TYPE))
  
  training <- na.omit(training)
  
  training <- mutate(training,
                     log_dist_cfa = log(dist_cfa),
                     log_dist_camp = log(dist_camp),
                     log_dist_road = log(dist_road),
                     COVER = factor(COVER),
                     HEIGHT = factor(HEIGHT))
  
  training <- rename(training, cause = new_cause)
  training <- mutate(training,
                     cause = fct_relevel(cause,
                                         "lightning",
                                         "accident",
                                         "arson",
                                         "burning_off"))
  
  training <- na.omit(training)
  
  training <- select(training, -year, -dist_road, -dist_cfa, -dist_camp, -FIRE_START)
  
  inTraining <- createDataPartition(training$cause, p = .8, list = FALSE)[,1]
  train_set <- training[inTraining,]
  test_set  <- training[-inTraining,]
  
  rf_best_features <- readRDS("data/RF_best_features.rds")

  set.seed(123456)
  
  lime_sample <- sample(1:length(test_set$cause), 1000)
  
  training2 <- select(training, !!rf_best_features$feature)
  train_set <- training2[inTraining,]
  test_set  <- training2[-inTraining,]
  
  library(lime)
  
  rf_model <- readRDS("data/Final_model.rds")
  
  explainer <- lime(train_set, rf_model, bin_continuous = FALSE)
  
  explaination <- explain(test_set[lime_sample, ],
                          explainer,
                          n_labels = 4,
                          n_features = 10)
  
  name_dict <- list("log_dist_camp" = "Log distance to the nearest camping site",
       "log_dist_cfa" = "Log distance to the nearest CFA station",
       "log_dist_road" = "Log distance to the nearest road",
       "lon" = "longitude",
       "lat" = "Latitude",
       "ase180" = "Average solar exposure in past 180 days",
       "arf360" = "Average rainfall in past 360 days",
       "ase90" = "Average solar exposure in past 90 days",
       "aws_m24" = "Average wind speed in past 24 months",
       "aws_m12" = "Average wind speed in past 12 months")
  
  explaination %>%
  mutate(label = factor(tools::toTitleCase(label), levels = c("Lightning", "Accident", "Arson", "Buring_off"))) %>%
  ggplot() +
    geom_boxplot(aes(feature_weight, feature), outlier.size = 0.5) +
    geom_vline(xintercept = 0, col = "red") +
    ylab("feature") +
    facet_wrap(~label, ncol = 2) +
    xlab("Feature Weight") +
    ylab("Feature")

  
  saveRDS(explaination, file = "data/lime_ex2")
  
  
} else {
  

  explaination <- readRDS("data/lime_ex2")
  
  name_dict <- list("log_dist_camp" = "Log distance to the nearest camping site",
       "log_dist_cfa" = "Log distance to the nearest CFA station",
       "log_dist_road" = "Log distance to the nearest road",
       "lon" = "longitude",
       "lat" = "Latitude",
       "ase180" = "Average solar exposure in past 180 days",
       "arf360" = "Average rainfall in past 360 days",
       "ase90" = "Average solar exposure in past 90 days",
       "aws_m24" = "Average wind speed in past 24 months",
       "aws_m12" = "Average wind speed in past 12 months")
  
  explaination %>%
  mutate(label = factor(tools::toTitleCase(label), levels = c("Lightning", "Accident", "Arson", "Burning_off"))) %>%
  ggplot() +
    geom_boxplot(aes(feature_weight, feature), outlier.size = 0.5) +
    geom_vline(xintercept = 0, col = "red") +
    ylab("feature") +
    facet_wrap(~label, ncol = 2) +
    xlab("Feature Weight") +
    ylab("Feature")
  
}


```


## Predicting ignition causes for 2019-2020 season

A fitted random forest model, along with covariate data in 2019-2020 bushfire season, can be used to produce the prediction of the cause of the bushfires in Victoria during the 2019-2020 season. Figure \ref{fig:pred2019} shows the prediction produced by the final model. And Table \ref{tab:predictionsummary} summarizes the prediction. According to the prediction, most majority of the bushfires in 2019-2020 season are caused by lightning. However, there are 138 bushfires caused by accidents which take up 14% of the total fires. Majority of the accident-caused bushfires are ignited in March. Besides, 37 bushfires are caused by arsonists. There was a noticeable bushfire at French island in January, which caused serious damage to the koala habitat. Our model predicts its cause is arson. Very few planned burns are predicted after October 2019 which suggests the correctness of our model. It is because fire restrictions normally start in October. 

Furthermore, we provide a map with 0.2-degree spatial resolution for quick decision making, which is given in Figure \ref{fig:gridpred}. If the investigator observes a new ignition between December and March and the weather condition is similar to the 2019-2020 bushfire season, prediction of the cause can be made immediately by checking Figure \ref{fig:gridpred}. 

Other than that, Figure \ref{fig:gridpred} also reveals the temporal and spatial characteristics of different causes. Probability of lightning-caused bushfire is almost time and spatial invariant from October to March. Bushfires start in the east and the west of Victoria are likely to be predicted by our model as lightning-ignited bushfires. The probability of accident-caused bushfire evolves by time. In October, only the north of Victoria has a high probability of accident-caused bushfire, but the high probability region spreads to the south over time and reaches its peak in March. Probability of arson-caused bushfire excluding the Melbourne region decreases as time goes from October to March. Planned burns only occur in October and November.




```{r pred2019, fig.cap="Prediction of the cause of bushfire ignitions in Victoria during the 2019-2020 season produced by the final model. Majority of the bushfires are caused by lightning. Most of the Planned burns occurred before November. Bushfires caused by arson mainly occur in December 2019 and March 2020. A large proportion of bushfires in March 2020 are predicted to be accident-caused.", fig.height=8}

library(ggthemes)
library(rnaturalearth)
au_map <- ne_states(country = 'Australia', returnclass = 'sf')
vic_map <- au_map[7,]
read_csv("data/prediction_2019-2020.csv") %>%
  mutate(cause = factor(tools::toTitleCase(cause), levels = c("Lightning", "Accident", "Arson", "Burning_off"))) %>%
  ggplot() +
  geom_sf(data = vic_map, fill = "white") +
  geom_point(aes(lon, lat, col = cause), size = 1) +
  theme_map(base_size = 10) +
  theme(legend.position = "none") +
  scale_color_brewer(palette = "RdBu") +
  facet_grid(factor(month.abb[month(time)], levels = c("Oct", "Nov", "Dec", "Jan", "Feb", "Mar"))~cause)
  
```




```{r}
read_csv("data/prediction_2019-2020.csv") %>%
  mutate(cause = factor(tools::toTitleCase(cause), levels = c("Lightning", "Accident", "Arson", "Burning_off"))) %>%
  mutate(month = factor(month.abb[month(time)], levels = c("Oct", "Nov", "Dec", "Jan", "Feb", "Mar"))) %>%
  group_by(cause, month) %>%
  count() %>%
  ungroup() %>%
  rename(Cause = cause) %>%
  spread(month, n) -> temp

temp[is.na(temp)] <- 0

temp$Total <- temp$Oct + temp$Nov + temp$Dec + temp$Jan + temp$Feb + temp$Mar
temp$Total <- paste0(temp$Total, " (", round(temp$Total/sum(temp$Total), 2), "%)")

temp %>%
  knitr::kable(booktabs = T, 
               label = "predictionsummary",
               caption = "A summary of the predicted causes of 2019-2020 Australia bushfires. Our model predicts 82\\% of the bushfires were ignited by lightning, 14\\% were ignited by accident, and only 4\\% and 1\\% were arson and planned burns respectively.")
```




```{r gridpred, fig.cap="A map with 0.2-degree spatial resolution for quick decision making of the cause of the bushfire during a bushfire season. This map is based on the assumption that the long-run weather condition of the new ignition is similar to 2019-2020 Australia bushfire season. Users need to match the location of the observed ignition and the observed date with the map to obtain the prediction for each cause. The darker the region the higher the probability.", fig.height=8, fig.width=6.5}
lat <- seq(34, 39, 0.2)
lat <- -lat
lon <- seq(141, 150, 0.2)

grids <- expand.grid(lat, lon)


rect <- function(x){

  # left top
  lat1 <- x[1]
  lon1 <- x[2]

  # right top
  lat2 <- x[1]
  lon2 <- x[2]+0.2

  # right bottom
  lat3 <- x[1]-0.2
  lon3 <- x[2]+0.2

  # left bottom
  lat4 <- x[1]-0.2
  lon4 <- x[2]

  st_sfc(st_polygon(list(matrix(c(lon1,lat1,lon2,lat2,lon3,lat3,lon4,lat4,lon1,lat1), ncol =2, byrow = TRUE))))

}

rect_list <- apply(grids[1:nrow(grids),],1,rect)

rect_list <- do.call(c, rect_list)

st_crs(rect_list) <- 4326

indexes <- st_intersects(vic_map$geometry, rect_list)[[1]]
rect_list <- rect_list[indexes]



sim <- read_csv("data/prediction_2019-2020_simulation.csv")

sim2 <- st_as_sf(sim, coords = c("lon", "lat"), crs = 4326)


rect_index <- st_intersects(rect_list, sim2)

sim$rect <- 0
for (i in 1:length(indexes)){
  sim$rect[rect_index[[i]]] <- i
}

rect_list <- as.data.frame(rect_list)
rect_list <- mutate(rect_list, rect = 1:nrow(rect_list))


temp <- sim %>%
  group_by(rect, month, cause) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  spread(cause, count)

temp[is.na(temp)] <- 0
  
rect_list <- temp %>%
  right_join(rect_list, by = c("rect"))

rect_list <- rect_list %>%
  gather(key = "cause", value = "count", lightning, accident, arson, burning_off) %>%
  mutate(cause = factor(tools::toTitleCase(cause), levels = c("Lightning", "Accident", "Arson", "Burning_off")))

ggplot() +
  geom_sf(data = rect_list, aes(fill = count/10, geometry = geometry), col = NA) +
  geom_sf(data = vic_map, fill = NA) +
  facet_grid(factor(month.abb[month], levels = c("Oct", "Nov", "Dec", "Jan", "Feb", "Mar"))~cause) +
  scale_fill_distiller(palette = "Reds", direction = 1, limits = c(0,1)) +
  theme_map() +
  theme(legend.position = "bottom") +
  labs(fill = "Probability")
```

## Fire risk map

Our predictive model can produce the probability of the bushfire ignited by a certain type of sources $P(M|S,\mathcal{F})$, but it does not provide us with the risk of the bushfire ignited by that source $P(M,S|\mathcal{F})$, where $M$ is the method of the bushfire ignition, $M \in \{\text{lightning, arson, accident, burning off}\}$, $S$ is the bushfire is ignited and $\mathcal{F}$ is the supplementary information about the bushfire ignition, such as location, time, weather conditions, distribution of vegetation.

Knowing $P(M|S,\mathcal{F})$ is useful for bushfire investigation but is not particularly helpful for bushfire management. In general, decision makers want to know the overall risk $P(S|\mathcal{F})$ and the risk of the bushfire ignited by a certain method $P(M,S|\mathcal{F})$.

A possible method to obtain $P(M,S|\mathcal{F})$ is by using the decomposition of the conditional probability:

$$P(M,S|\mathcal{F}) = P(M|S,\mathcal{F})P(S|\mathcal{F})$$
We will demonstrate how this works by using a simple method to estimate $P(S|\mathcal{F})$ and eventually yield $P(M,S|\mathcal{F})$ in the rest of the section.

If we only consider month $MON$ and location $L$ as $\mathcal{F}$, a reasonable estimator of
the probability of at least one bushfire ignited in a region $l$ in a certain month $mon$ is the relative frequency calculated from historical records:

$$P(S|MON = mon, L = l) = \frac{\sum_{i=1}^{N}I(\text{bushfire ignition occurred in month } mon, \text{year } y_i)}{N}$$, where $N$ is the number of years in the historical records and $y_i$ is the $i$ th year of the historical records.


Figure \ref{fig:mcsim} shows the estimate of $P(S|MON = mon, L = l)$ using 20 years of historical records. If we assume the weather conditions are similar to 2019-2020 bushfire season, $P(M|S,MON = mon, L = l)$ can be produced by our random forest model and the joint probability $P(L,S|\mathcal{F})$ can then be calculated. Figure \ref{fig:posterior} shows the final outcome. From the result, we notice that January is the most dangerous month in terms of lightning-caused bushfire and the east of Victoria is the high risk area. The risk of arson-caused bushfire is concentrated in a relatively small region near Melbourne.  

```{r mcsim, fig.cap="A map of the relative frequency of the bushfire ignition occurred in a region in the past 20 years. It is an estimate of the probability of at least one bushfire ignition occur in a given region and a given month."}
 set.seed(10086)
  
  # Read in training data
training <- read_csv("training.csv")
  
  training <- training %>%
    filter(!CAUSE %in% c("BURNING BUILDING",
                         "WASTE DISPOSAL, INDUSTRIAL, SAWMILL, TIP",
                         "WASTE DISPOSAL, DOMESTIC",
                         "BURNING VEHICLE, MACHINE",
                         "BURNING BUILDING")) %>%
    filter(new_cause != "other") %>%
    filter(new_cause != "relight")
  
  
  training <- select(training, -c(EVENTID:FIRE_NUM), -id, -CAUSE, -FOREST, -FOR_CODE, -FOR_CAT)
  
  training <- mutate(training,
                     year = factor(year(FIRE_START)),
                     month = factor(month(FIRE_START), levels = c(10,11,12,1,2,3)),
                     day = factor(day(FIRE_START), levels = c(1:31)),
                     wod = factor(wday(FIRE_START), levels = c(1:7)))
  
  training <- filter(training, month %in% c(10,11,12,1,2,3))
  
  training <- na.omit(training)
  
  training <- mutate(training, new_cause = ifelse(new_cause == "accidental_human", "accident", new_cause)) %>%
    mutate(new_cause = ifelse(new_cause == "burning_off_human", "burning_off", new_cause)) %>%
    mutate(new_cause = factor(new_cause)) %>%
    mutate(FOR_TYPE = factor(FOR_TYPE))
  
  training <- na.omit(training)
  
  training <- mutate(training,
                     log_dist_cfa = log(dist_cfa),
                     log_dist_camp = log(dist_camp),
                     log_dist_road = log(dist_road),
                     COVER = factor(COVER),
                     HEIGHT = factor(HEIGHT))
  
  training <- rename(training, cause = new_cause)
  training <- mutate(training,
                     cause = fct_relevel(cause,
                                         "lightning",
                                         "accident",
                                         "arson",
                                         "burning_off"))
  
  training <- na.omit(training)
  
  training <- select(training, -dist_road, -dist_cfa, -dist_camp, -FIRE_START)
  

  
library(sf)
library(rnaturalearth)
au_map <- ne_states(country = 'Australia', returnclass = 'sf')
vic_map <- au_map[7,]

lat <- seq(34, 39, 0.2)
lat <- -lat
lon <- seq(141, 150, 0.2)

grids <- expand.grid(lat, lon)


rect <- function(x){

  # left top
  lat1 <- x[1]
  lon1 <- x[2]

  # right top
  lat2 <- x[1]
  lon2 <- x[2]+0.2

  # right bottom
  lat3 <- x[1]-0.2
  lon3 <- x[2]+0.2

  # left bottom
  lat4 <- x[1]-0.2
  lon4 <- x[2]

  st_sfc(st_polygon(list(matrix(c(lon1,lat1,lon2,lat2,lon3,lat3,lon4,lat4,lon1,lat1), ncol =2, byrow = TRUE))))

}

rect_list <- apply(grids[1:nrow(grids),],1,rect)

rect_list <- do.call(c, rect_list)

st_crs(rect_list) <- 4326

indexes <- st_intersects(vic_map$geometry, rect_list)[[1]]
rect_list <- rect_list[indexes]

training2 <- st_as_sf(training, coords = c("lon", "lat"), crs = 4326)

indexes <- st_intersects(rect_list, training2)

training$rect <- 0
for (i in 1:length(indexes)){
  training$rect[indexes[[i]]] <- i
}

rect_list <- as.data.frame(rect_list)
rect_list <- mutate(rect_list, rect = 1:nrow(rect_list), month = 10)

rect_list <- rect_list %>%
  bind_rows(mutate(rect_list, month = 11)) %>%
  bind_rows(mutate(rect_list, month = 12)) %>%
  bind_rows(mutate(rect_list, month = 1)) %>%
  bind_rows(mutate(rect_list, month = 2)) %>%
  bind_rows(mutate(rect_list, month = 3)) %>%
  mutate(month = factor(month, levels = c("10", "11", "12", "1", "2", "3")))

temp <- training %>%
  group_by(rect, year, month) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  group_by(rect, month) %>%
  summarise(count = n())

rect_list <- temp %>%
  right_join(rect_list, by = c("rect", "month")) %>%
  mutate(count = ifelse(is.na(count), 0, count))

ggplot() +
  geom_sf(data = rect_list, aes(geometry = geometry, fill = count/19), col = NA) +
  geom_sf(data = vic_map, fill = NA) +
  facet_wrap(~fct_recode(month, "Oct" = "10", "Nov" = "11", "Dec" = "12", "Jan" = "1", "Feb" = "2", "Mar" = "3")) +
  theme_map() +
  theme(legend.position = "bottom") +
  scale_fill_distiller(palette = "Reds", direction = 1, limits = c(0,1), na.value = "grey") +
  labs(fill = "Relative frequency")

rect_list_st <- rect_list

```

```{r posterior, fig.cap="The joint probability of bushfire ignited by different methods in different months. The east of Victoria has the highest risk of lightning-caused bushfire in January. Arson risk is concentrated around Melbourne.", fig.height=8, fig.width=6.5}
lat <- seq(34, 39, 0.2)
lat <- -lat
lon <- seq(141, 150, 0.2)

grids <- expand.grid(lat, lon)


rect <- function(x){

  # left top
  lat1 <- x[1]
  lon1 <- x[2]

  # right top
  lat2 <- x[1]
  lon2 <- x[2]+0.2

  # right bottom
  lat3 <- x[1]-0.2
  lon3 <- x[2]+0.2

  # left bottom
  lat4 <- x[1]-0.2
  lon4 <- x[2]

  st_sfc(st_polygon(list(matrix(c(lon1,lat1,lon2,lat2,lon3,lat3,lon4,lat4,lon1,lat1), ncol =2, byrow = TRUE))))

}

rect_list <- apply(grids[1:nrow(grids),],1,rect)

rect_list <- do.call(c, rect_list)

st_crs(rect_list) <- 4326

indexes <- st_intersects(vic_map$geometry, rect_list)[[1]]
rect_list <- rect_list[indexes]



sim <- read_csv("data/prediction_2019-2020_simulation.csv")

sim2 <- st_as_sf(sim, coords = c("lon", "lat"), crs = 4326)


rect_index <- st_intersects(rect_list, sim2)

sim$rect <- 0
for (i in 1:length(indexes)){
  sim$rect[rect_index[[i]]] <- i
}

rect_list <- as.data.frame(rect_list)
rect_list <- mutate(rect_list, rect = 1:nrow(rect_list))


temp <- sim %>%
  group_by(rect, month, cause) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  spread(cause, count)

temp[is.na(temp)] <- 0
  
rect_list <- temp %>%
  right_join(rect_list, by = c("rect"))

rect_list <- rect_list %>%
  gather(key = "cause", value = "count", lightning, accident, arson, burning_off) %>%
  mutate(cause = factor(tools::toTitleCase(cause), levels = c("Lightning", "Accident", "Arson", "Burning_off")))


# rect_list <- filter(rect_list, cause == "Lightning")
rect_list_st <- rename(rect_list_st, past_count = count)

rect_list_st <- mutate(rect_list_st, month = as.numeric(as.character(month)))


rect_list <- left_join(rect_list, select(rect_list_st, -geometry), by = c("rect", "month"))


rect_list <- mutate(rect_list, poster = (count/10)*(past_count/19))

ggplot() +
  geom_sf(data = rect_list, aes(geometry = geometry, fill = poster), col = NA) +
  geom_sf(data = vic_map, fill = NA) +
  facet_grid(factor(month.abb[month], levels = c("Oct", "Nov", "Dec", "Jan", "Feb", "Mar"))~cause) +
  theme_map() +
  theme(legend.position = "bottom") +
  scale_fill_distiller(palette = "Reds", direction = 1, limits = c(0,1)) +
  labs(fill = "Probability")
```
